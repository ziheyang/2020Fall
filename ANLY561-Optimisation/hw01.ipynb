{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "## Due September 4th before Midnight\n",
    "\n",
    "\n",
    "## Readings: Lecture 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In lecture, we discussed minimization programs, but maximization programs are another possibility. Given a feasible region $\\mathcal{X}$ and a **reward** function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$, a maximization program\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "seeks a **maximizer** of $f$ over $\\mathcal{X}$ (a point $x^\\ast$ satisfying $f(x)\\leq f(x^\\ast)$ for all $x\\in\\mathcal{X}$). Just as in minimization, we say that a minimizer of $f$ over $\\mathcal{X}$ is a **solution** to the program, or **solves** the program.\n",
    "\n",
    "For this problem, show that $x^\\ast$ is a solution to\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "if and only if $x^\\ast$ is a solution to\n",
    "\n",
    "$$\n",
    "\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}.\n",
    "$$\n",
    "\n",
    "**Hint**: Recall that, to prove \"A if and only if B\", you need to prove two separate statements:\n",
    "1. If A, then B.\n",
    "2. If B, then A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x^\\ast$ is a solution to $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$, then $f(x)\\leq f(x^\\ast)$ for all $x\\in\\mathcal{X}$. <br>\n",
    "Next, multiply -1 to both side of the inequation, we have $-f(x)\\geq -f(x^\\ast)$ for all $x\\in\\mathcal{X}$, which by definition, $x^\\ast$ is the minimizer of $-f$ over $\\mathcal{X}$. <br>\n",
    "Therefore, $x^\\ast$ is a solution to $\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}$.\n",
    "\n",
    "If $x^\\ast$ is a solution to $\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}$, then $-f(x)\\geq -f(x^\\ast)$ for all $x\\in\\mathcal{X}$.  <br>\n",
    "Next, multiply -1 to both side of the inequation, we have $f(x)\\leq f(x^\\ast)$ for all $x\\in\\mathcal{X}$, which by definition, $x^\\ast$ is the maximizer of $f$ over $\\mathcal{X}$.  <br>\n",
    "Therefore, $x^\\ast$ is a solution to $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$.\n",
    "\n",
    "Thus, the statement is true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The fact that one can replace any maximization problem with a minimization problem means that we only need to consider minimization problems in general. This is called the **reflection principle** because $-f(x)$ reflects $f(x)$ across the $x$ axis. Because of the reflection principle, when we talk about optimization programs in this course, we will almost exclusively be discussing minimization programs. \n",
    "\n",
    "This also illustrates how it sometimes helps to modify a function $f$ to find a solution to a program. For two programs\n",
    "$$\n",
    "(P_1):\\:\\min f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "(P_2):\\:\\min g(x)\\text{ subject to } x\\in\\mathcal{X},\n",
    "$$\n",
    "whenever it holds that $x^\\ast$ solves $P_1$ if and only $x^\\ast$ solves $P_2$, we say that the programs $P_1$ and $P_2$ are **equivalent**. We also say that these programs are equivalent if this statement holds, but either of the $\\min$'s are replaced with $\\max$'s. \n",
    "\n",
    "For example, this means that $\\max f(x)$ and $\\min -f(x)$ are equivalent programs by the reflection principle.\n",
    "\n",
    "### Part A\n",
    "\n",
    "For this part, show that for any **monotonically decreasing** or **order-reversing** function $h:\\mathbb{R}\\rightarrow\\mathbb{R}$ ($h(x)<h(y)$ whenever $y<x$),\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "are equivalent programs. \n",
    "\n",
    "### Part B\n",
    "\n",
    "Also show that, for any **monotonically increasing** or **order-preserving** function $\\widetilde{h}:\\mathbb{R}\\rightarrow\\mathbb{R}$ ($h(x)<h(y)$ whenever $y<x$),\n",
    "\n",
    "$$\n",
    "\\min f(x)\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\min \\widetilde{h}(f(x))\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "If $x^\\ast$ is a maximizer to $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$,\n",
    "then, $f(x)\\leq f(x^\\ast)$ for all $x\\in\\mathcal{X}$.  <br>\n",
    "Since $h(x)$ is a monotonically decreasing function, then for $h:\\mathbb{R}\\rightarrow\\mathbb{R}$, if $f(x)\\leq f(x^\\ast)$, then $h(f(x))\\geq h(f(x^\\ast))$,. <br>\n",
    "Therefore, $x^\\ast$ is a the solution of $\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}$. <br>\n",
    "\n",
    "If $x^\\ast$ is a minimizer to $\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}$,\n",
    "then, $h(f(x))\\geq h(f(x^\\ast))$.  <br>\n",
    "Since $h(x)$ is a monotonically decreasing function, then for $h:\\mathbb{R}\\rightarrow\\mathbb{R}$, if $h(f(x))\\geq h(f(x^\\ast))$, then $f(x)\\leq f(x^\\ast)$. <br>\n",
    "Therefore, $x^\\ast$ is a the solution of $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$. <br>\n",
    "\n",
    "Thus, the two programs are equivalent.  \n",
    "\n",
    "#### Part B\n",
    "Suppose $x^\\ast$ is a minimizer to $\\min f(x)\\text{ subject to }x\\in\\mathcal{X}$, then $f(x^\\ast)\\leq f(x)$ for all $x\\in\\mathcal{X}$. <br>\n",
    "Since $\\widetilde{h}(x)$ is a monotonically increasing function, then for all $h:\\mathbb{R}\\rightarrow\\mathbb{R}$. If $f(x^\\ast)\\leq f(x)$, then $\\widetilde{h}(f(x^\\ast))\\leq \\widetilde{h}(f(x))$.  <br>\n",
    "Therefore, $x^\\ast$ is a solution to $\\min f(x)\\text{ subject to }x\\in\\mathcal{X}$, and it is also the solution to $\\min \\widetilde{h}(f(x))\\text{ subject to }x\\in\\mathcal{X}$. <br>\n",
    "The two programs are equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "\n",
    "Linear regression is often introduced by setting\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where each $\\varepsilon_i$ is an unknown number drawn from a normal distribution with mean $0$ and known variance $\\sigma^2$. That is, the p.d.f. of $\\varepsilon_i$ is \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\varepsilon_i^2/2\\sigma^2}.\n",
    "$$\n",
    "\n",
    "Because of the first inequality, we can rewrite this p.d.f. as the **likelihood** of $\\beta_0,\\beta_1$ given the data $(x_i, y_i)$:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0,\\beta_1; (x_i, y_i)) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "Since the $\\varepsilon_i$'s are independent the p.d.f. of all the errors is simple the product of p.d.f.'s, and hence the likelihood of observing the data set is simply the product of likelihoods:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0,\\beta_1; \\{(x_i, y_i)\\}_{i=1}^N) = \\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "The **maximum likelihood principle** states that we should find $\\beta_0$ and $\\beta_1$ which maximize the likelihood function $\\ell$.\n",
    "\n",
    "Show that\n",
    "$$\n",
    "\\max \\ell(\\beta_0, \\beta_1)\\text{ subject to }(\\beta_0,\\beta_1)\\in\\mathbb{R}^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\min \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2\n",
    "$$\n",
    "are equivalent programs. Here, we have used $y$ and $X$ from the lecture, and we have suppressed the data dependency of $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\beta_0$ and $\\beta_1$ are the solutions to the likelihood funtion $\\ell$:\n",
    "$$\n",
    "\\max \\ell(\\beta_0, \\beta_1)\\text{ subject to }(\\beta_0,\\beta_1)\\in\\mathbb{R}^2\n",
    "$$\n",
    "If we plug the likelihood function in \n",
    "$$\n",
    "\\begin{align}\n",
    "\\max \\ell(\\beta_0, \\beta_1) &= \\max \\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}} \\\\\n",
    "& = \\max (\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^N e^{-\\frac{\\sum_{i=1}^N(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Since $(\\frac{1}{\\sqrt{2\\pi\\sigma^2}})^N$ is a constant, therefore, we only want to maximize $$ e^{-\\frac{\\sum_{i=1}^N(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}} $$\n",
    "From the class lecture, we already know that \n",
    "$$\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 = \\Vert y - X\\beta\\Vert^2 $$\n",
    "Therefore, we want to maximize $$ e^{-\\frac{\\Vert y - X\\beta\\Vert^2} {2\\sigma^2}} $$\n",
    "Take the natural logarithm of this term, we got maximization of $$ {-\\frac{\\Vert y - X\\beta\\Vert^2} {2\\sigma^2}}$$\n",
    "Since $ \\frac {1} {2\\sigma^2}$ is just a constance, we could represent it as $ \\frac {1} {N}$. \n",
    "So we need to find the solution of $$ \\max -\\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2$$\n",
    "This is equivalent to $$ \\min \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2$$\n",
    "Thus, these are equvalent programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Prove the parallelogram identity:\n",
    "\n",
    "$$\n",
    "x^Ty = \\frac{1}{4}\\left(\\Vert x+y\\Vert^2-\\Vert x-y\\Vert^2\\right)\n",
    "$$\n",
    "\n",
    "for all $x,y\\in\\mathbb{R}^d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "x^Ty &= \\frac{1}{4}\\left(\\Vert x+y\\Vert^2-\\Vert x-y\\Vert^2\\right)\\\\\n",
    "&= \\frac{1}{4} \\left((x+y)^T(x+y) - (x-y)^T(x-y) \\right) \\\\\n",
    "&= \\frac{1}{4} \\left((x^T+y^T)(x+y)- (x^T-y^T)(x-y) \\right) \\\\\n",
    "&= \\frac{1}{4} \\left(x^Tx +x^Ty +y^Tx+y^Ty - (x^Tx -x^Ty -y^Tx+y^Ty) \\right) \\\\\n",
    "&= \\frac{1}{4} \\left(x^Tx +x^Ty +y^Tx+y^Ty - x^Tx +x^Ty +y^Tx-y^Ty \\right) \\\\\n",
    "&= \\frac{1}{4} \\left(x^Ty +y^Tx +x^Ty +y^Tx\\right) \\\\  \n",
    "&= \\frac{1}{4} \\left(x^Ty +x^Ty +x^Ty +x^Ty\\right) \\\\\n",
    "&= \\frac{1}{4} \\left(4x^Ty \\right) \\\\\n",
    "&= x^Ty \n",
    "\\end{align}\n",
    "$$\n",
    "Thus, the parallelogram identity is proved.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Another benefit of our reformulation of the $SSE$ function is that it is now dimension independent. That is, if we are given data $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i\\in\\mathbb{R}^d$ and $y_i\\in\\mathbb{R}$ for each $i=1,\\ldots, N$ and we form the higher-order $SSE$ function by defining\n",
    "\n",
    "$$\n",
    "SSE(\\beta_0,\\beta_1,\\ldots, \\beta_d) = \\sum_{i=1}^N(y_i-\\beta_0-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\cdots-\\beta_d x_{i,d})^2\n",
    "$$\n",
    "\n",
    "with $x_{i,j}$ the $j$th entry of $x_i$, then\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = \\Vert y - X\\beta\\Vert^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y=\\begin{pmatrix}y_1\\\\ y_2\\\\\\vdots\\\\ y_N\\end{pmatrix},\\:\\beta=\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_d\n",
    "\\end{pmatrix},\\text{ and } X=\\begin{pmatrix} 1 & x_{1, 1} & x_{1,2} & \\cdots & x_{1,d}\\\\1 & x_{2, 1} & x_{2,2} & \\cdots & x_{2,d}\\\\\\vdots & \\vdots& \\vdots &\\ddots & \\vdots\\\\1 & x_{N, 1} & x_{N,2} & \\cdots & x_{N,d}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Part A\n",
    "\n",
    "Modify the code from class to compute the $\\beta^\\ast\\in\\mathbb{R}^{14}$ (the $0$th entry is the constant $\\beta_0$ and the other entries correpond to the $13$ features of the dataset) solving the least squares problem. Print the $\\beta^\\ast$'s and discuss your findings.\n",
    "\n",
    "### Part B\n",
    "\n",
    "Numerically verify that the $\\beta^\\ast$ satisfies the normal equations. That is, use python code to show that $X^TX\\beta^\\ast - X^T y\\approx 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution to Problem 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  (506, 13)\n",
      "y shape:  (506,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zihe/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.64594884e+01, -1.08011358e-01,  4.64204584e-02,  2.05586264e-02,\n",
       "        2.68673382e+00, -1.77666112e+01,  3.80986521e+00,  6.92224640e-04,\n",
       "       -1.47556685e+00,  3.06049479e-01, -1.23345939e-02, -9.52747232e-01,\n",
       "        9.31168327e-03, -5.24758378e-01])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston # Import the Boston housing dataset loader\n",
    "\n",
    "boston = load_boston() # Instantiate the python map object containing the Boston housing dataset and information\n",
    "x = boston.data # get all the 13 columns \n",
    "print(\"x shape: \", x.shape) # Show the number of points of the data\n",
    "y = boston.target # Set the y_i's to be the nitrogen oxide levels\n",
    "print(\"y shape: \", y.shape) # Show the number of responses\n",
    "\n",
    "one = np.ones((506,1))\n",
    "x = np.reshape(x, (506, 13))\n",
    "\n",
    "X = np.concatenate((one, x), axis=1) # Form the design matrix for the NO_2 levels \n",
    "beta = np.linalg.lstsq(X, y)[0] # Use the linear algebra least squares routine to compute the beta's\n",
    "tilde_y = X @ beta # Form the approximate responses\n",
    "\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beta value is listed above. The first beta value contributes to the one nuisance. So starting from the second beta value, the ZN, INDUS, CHAS, RM, AGE, RAD and B have a positive impact on the value of target. Meanwhile, the CRIM, NOX, DIS, TAX, PTRATIO, LSTAT give a negative impact on target. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.81898940e-12,  2.18278728e-11,  0.00000000e+00, -3.78349796e-10,\n",
       "       -1.36424205e-12, -2.72848411e-12, -5.82076609e-11, -1.39698386e-09,\n",
       "       -5.09317033e-11, -4.36557457e-11,  5.58793545e-09, -2.91038305e-10,\n",
       "       -1.86264515e-09, -1.30967237e-10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.matmul(X.transpose() , X)\n",
    "firstTerm = np.matmul(a, beta) \n",
    "secondTerm = np.matmul(X.transpose() , y) \n",
    "firstTerm - secondTerm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python result shows $X^TX\\beta^\\ast - X^T y\\approx 0$ is very close to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
