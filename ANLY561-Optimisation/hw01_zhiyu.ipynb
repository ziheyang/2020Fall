{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "## Due September 11th before Midnight\n",
    "\n",
    "\n",
    "## Readings: Lecture 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In lecture, we discussed minimization programs, but maximization programs are another possibility. Given a feasible region $\\mathcal{X}$ and a **reward** function $f:\\mathcal{X}\\rightarrow\\mathbb{R}$, a maximization program\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "seeks a **maximizer** of $f$ over $\\mathcal{X}$ (a point $x^\\ast$ satisfying $f(x)\\leq f(x^\\ast)$ for all $x\\in\\mathcal{X}$). Just as in minimization, we say that a minimizer of $f$ over $\\mathcal{X}$ is a **solution** to the program, or **solves** the program.\n",
    "\n",
    "For this problem, show that $x^\\ast$ is a solution to\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "if and only if $x^\\ast$ is a solution to\n",
    "\n",
    "$$\n",
    "\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}.\n",
    "$$\n",
    "\n",
    "**Hint**: Recall that, to prove \"A if and only if B\", you need to prove two separate statements:\n",
    "1. If A, then B.\n",
    "2. If B, then A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 1\n",
    "\n",
    "First, I will prove \"*if* $\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}$ *then* $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$\".\n",
    "\n",
    "By the definition of \"solution\", we know that $x^*$ is a minimizer of $-f$ over $\\mathcal{X}$. \n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "-f(x^*) \\leq -f(x) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "f(x^*) \\geq f(x) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "Thus, $x^*$ is a maximizer of $f$ over $\\mathcal{X}$, meaning that $x^*$ is a solution to \n",
    "\n",
    "$$\n",
    "\\max f(x) \\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "Now I will prove \"*if* $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$ *then* $\\min -f(x)\\text{ subject to } x\\in\\mathcal{X}$\".\n",
    "\n",
    "By the definition of \"solution\", we know that $x^*$ is a maximizer of $f$ over $\\mathcal{X}$.\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "f(x^*) \\geq f(x) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "-f(x^*) \\leq -f(x) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "Thus $x^*$ is a minimizer of $-f$ over $\\mathcal{X}$, meaning that $x^*$ is a solution to \n",
    "\n",
    "$$\n",
    "\\min -f(x) \\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "In conclusion, $x^*$ is a solution to\n",
    "\n",
    "$$\n",
    "\\max f(x) \\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "if and only if $x^*$ is a solution to \n",
    "\n",
    "$$\n",
    "\\min -f(x) \\text{ subject to } x\\in\\mathcal{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The fact that one can replace any maximization problem with a minimization problem means that we only need to consider minimization problems in general. This is called the **reflection principle** because $-f(x)$ reflects $f(x)$ across the $x$ axis. Because of the reflection principle, when we talk about optimization programs in this course, we will almost exclusively be discussing minimization programs. \n",
    "\n",
    "This also illustrates how it sometimes helps to modify a function $f$ to find a solution to a program. For two programs\n",
    "$$\n",
    "(P_1):\\:\\min f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "(P_2):\\:\\min g(x)\\text{ subject to } x\\in\\mathcal{X},\n",
    "$$\n",
    "whenever it holds that $x^\\ast$ solves $P_1$ if and only $x^\\ast$ solves $P_2$, we say that the programs $P_1$ and $P_2$ are **equivalent**. We also say that these programs are equivalent if this statement holds, but either of the $\\min$'s are replaced with $\\max$'s. \n",
    "\n",
    "For example, this means that $\\max f(x)$ and $\\min -f(x)$ are equivalent programs by the reflection principle.\n",
    "\n",
    "### Part A\n",
    "\n",
    "For this part, show that for any **monotonically decreasing** or **order-reversing** function $h:\\mathbb{R}\\rightarrow\\mathbb{R}$ ($h(x)<h(y)$ whenever $y<x$),\n",
    "\n",
    "$$\n",
    "\\max f(x)\\text{ subject to } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "are equivalent programs. \n",
    "\n",
    "### Part B\n",
    "\n",
    "Also show that, for any **monotonically increasing** or **order-preserving** function $\\widetilde{h}:\\mathbb{R}\\rightarrow\\mathbb{R}$ ($h(x)<h(y)$ whenever $y<x$),\n",
    "\n",
    "$$\n",
    "\\min f(x)\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\min \\widetilde{h}(f(x))\\text{ subject to }x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 2\n",
    "\n",
    "### Part A\n",
    "\n",
    "First, I will prove \"*if* $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$, *then* $\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}$\".\n",
    "\n",
    "Assume $x^* \\in\\mathcal{X}$ is a maximizer for $f$, meaning that\n",
    "\n",
    "$$\n",
    "f(x^*) \\geq f(x) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "Since $h$ is monotonically decreasing, \n",
    "\n",
    "$$\n",
    "h(f(x^*)) \\leq h(f(x)) \\text{ for all } x\\in\\mathcal{X} \n",
    "$$\n",
    "\n",
    "Thus, $x^*$ is also a minimizer for $h(f(x))$, meaning that $x^*$ also solves $P_2$.\n",
    "\n",
    "Now I will prove \"*if* $\\min h(f(x))\\text{ subject to }x\\in\\mathcal{X}$, *then* $\\max f(x)\\text{ subject to } x\\in\\mathcal{X}$\".\n",
    "\n",
    "Assume $x^* \\in\\mathcal{X}$ is a minimizer for $h(f(x))$, meaning that \n",
    "\n",
    "$$\n",
    "h(f(x^*)) \\leq h(f(x)) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "Since $h$ is monotonically decreasing, \n",
    "\n",
    "$$\n",
    "f(x^*) \\geq f(x) \\text{ for all } x\\in\\mathcal{X}\n",
    "$$\n",
    "\n",
    "Thus, $x^*$ is also a maximizer for $f(x)$, meaning that $x^*$ also solves $P_1$.\n",
    "\n",
    "So far, I've proved that $x^\\ast$ solves $P_1$ if and only $x^\\ast$ solves $P_2$. So $P_1$ and $P_2$ are equivalent.\n",
    "\n",
    "### Part B\n",
    "\n",
    "First, I will prove \"*if* $\\min f(x)\\text{ subject to } x\\in\\mathcal{X}$, *then* $\\min \\widetilde h(f(x))\\text{ subject to }x\\in\\mathcal{X}$\".\n",
    "\n",
    "Assume $x^* \\in \\mathcal{X}$ exists such that \n",
    "\n",
    "$$\n",
    "f(x^*) \\leq f(x)\n",
    "$$\n",
    "\n",
    "Since $\\widetilde h$ is monotonically increasing, \n",
    "\n",
    "$$\n",
    "\\widetilde h(f(x^*)) \\leq \\widetilde h(f(x))\n",
    "$$\n",
    "\n",
    "So $x^*$ ix a minimizer for $\\widetilde h(f(x))$ too, or that $x^*$ is also the solution to program $\\min \\widetilde h(f(x))$.\n",
    "\n",
    "On the other hand, the statement \"*if* $\\min \\widetilde h(f(x))\\text{ subject to }x\\in\\mathcal{X}$, *then* $\\min f(x)\\text{ subject to } x\\in\\mathcal{X}$\" also holds true because:\n",
    "\n",
    "Assume $x^* \\in \\mathcal{X}$ exists such that \n",
    "\n",
    "$$\n",
    "\\widetilde h(f(x^*)) \\leq \\widetilde h(f(x))\n",
    "$$\n",
    "\n",
    "Since $\\widetilde h$ is monotonically increasing, \n",
    "\n",
    "$$\n",
    "f(x^*) \\leq f(x)\n",
    "$$\n",
    "\n",
    "So $x^*$ ix a minimizer for $f(x)$ too, or that $x^*$ is also the solution to program $\\min f(x)$.\n",
    "\n",
    "In conclusion, the two programs $\\min f(x)\\text{ subject to }x\\in\\mathcal{X}$ and $\\min \\widetilde{h}(f(x))\\text{ subject to }x\\in\\mathcal{X}$ are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:\n",
    "\n",
    "Linear regression is often introduced by setting\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "where each $\\varepsilon_i$ is an unknown number drawn from a normal distribution with mean $0$ and known variance $\\sigma^2$. That is, the p.d.f. of $\\varepsilon_i$ is \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\varepsilon_i^2/2\\sigma^2}.\n",
    "$$\n",
    "\n",
    "Because of the first inequality, we can rewrite this p.d.f. as the **likelihood** of $\\beta_0,\\beta_1$ given the data $(x_i, y_i)$:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0,\\beta_1; (x_i, y_i)) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "Since the $\\varepsilon_i$'s are independent the p.d.f. of all the errors is simple the product of p.d.f.'s, and hence the likelihood of observing the data set is simply the product of likelihoods:\n",
    "\n",
    "$$\n",
    "\\ell(\\beta_0,\\beta_1; \\{(x_i, y_i)\\}_{i=1}^N) = \\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "The **maximum likelihood principle** states that we should find $\\beta_0$ and $\\beta_1$ which maximize the likelihood function $\\ell$.\n",
    "\n",
    "Show that\n",
    "$$\n",
    "\\max \\ell(\\beta_0, \\beta_1)\\text{ subject to }(\\beta_0,\\beta_1)\\in\\mathbb{R}^2\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\min \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2\n",
    "$$\n",
    "are equivalent programs. Here, we have used $y$ and $X$ from the lecture, and we have suppressed the data dependency of $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 3\n",
    "\n",
    "First, I will prove that \"*if* $\\max \\ell(\\beta_0, \\beta_1)\\text{ subject to }(\\beta_0,\\beta_1)\\in\\mathbb{R}^2$ is true, *then* $\\min \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2$ must be true too\".\n",
    "\n",
    "Assume $\\beta^*_0$ and $\\beta^*_1$ exist such that $\\ell(\\beta^*_0, \\beta^*_1) \\geq \\ell(\\beta_0, \\beta_1)$.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta^*_0-\\beta^*_1x_i)^2}{2\\sigma^2}} \\geq \\prod_{i=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Since $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ is a constant, we can remove it from both sides and get\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^N e^{-\\frac{(y_i-\\beta^*_0-\\beta^*_1x_i)^2}{2\\sigma^2}} \\geq \\prod_{i=1}^N e^{-\\frac{(y_i-\\beta_0-\\beta_1x_i)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^N (e^{-(y_i-\\beta^*_0-\\beta^*_1x_i)^2})^\\frac{1}{2\\sigma^2} \\geq \\prod_{i=1}^N (e^{-(y_i-\\beta_0-\\beta_1x_i)^2})^\\frac{1}{2\\sigma^2}\n",
    "$$\n",
    "\n",
    "Because $\\frac{1}{2\\sigma^2} > 0$, we can remove it from both sides as well:\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^N (e^{-(y_i-\\beta^*_0-\\beta^*_1x_i)^2}) \\geq \\prod_{i=1}^N (e^{-(y_i-\\beta_0-\\beta_1x_i)^2})\n",
    "$$\n",
    "\n",
    "Since $e^a \\times e^b = e^{a+b}$, we can rewrite the inequality above as\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N -(y_i-\\beta^*_0-\\beta^*_1x_i)^2 \\geq \\sum_{i=1}^N -(y_i-\\beta_0-\\beta_1x_i)^2\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N (y_i-\\beta^*_0-\\beta^*_1x_i)^2 \\leq \\sum_{i=1}^N (y_i-\\beta_0-\\beta_1x_i)^2\n",
    "$$\n",
    "\n",
    "In the lecture, we have proved that \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 = \\Vert y - X\\beta\\Vert^2\n",
    "$$\n",
    "\n",
    "so \n",
    "\n",
    "$$\n",
    "\\Vert y - X\\beta^*\\Vert^2 \\leq \\Vert y - X\\beta\\Vert^2\n",
    "$$\n",
    "\n",
    "where $\\beta^* = (\\beta^*_0, \\beta^*_1)$.\n",
    "\n",
    "Let function $f(\\beta) = \\Vert y - X\\beta\\Vert^2$ and let $\\beta^*$ be the minimizer of $f(\\beta)$.\n",
    "\n",
    "Also let function $h(f(\\beta)) = \\frac{1}{N} f(\\beta)$. Because $\\frac{1}{N} > 0$, function $h$ must be monotonically increasing.\n",
    "\n",
    "Therefore, from Problem 2 Part B, we know that if $\\beta^*$ is a minimizer of $f(\\beta)$, it must also be a minimizer of $h(f(\\beta))$.\n",
    "\n",
    "So far, we have proved that \"*if* $\\max \\ell(\\beta_0, \\beta_1)\\text{ subject to }(\\beta_0,\\beta_1)\\in\\mathbb{R}^2$ is true, *then* $\\min \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^2$ must be true too\". Now I will prove from the other side.\n",
    "\n",
    "Let $\\beta^*$ be the minimizer of function $\\frac{1}{N}\\Vert y-X\\beta\\Vert^2$, then we have\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\Vert y-X\\beta^*\\Vert^2 \\leq \\frac{1}{N}\\Vert y-X\\beta\\Vert^2\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N (y_i-\\beta^*_0-\\beta^*_1x_i)^2 \\leq \\sum_{i=1}^N (y_i-\\beta_0-\\beta_1x_i)^2\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N -(y_i-\\beta^*_0-\\beta^*_1x_i)^2 \\geq \\sum_{i=1}^N -(y_i-\\beta_0-\\beta_1x_i)^2\n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^N (e^{-(y_i-\\beta^*_0-\\beta^*_1x_i)^2}) \\geq \\prod_{i=1}^N (e^{-(y_i-\\beta_0-\\beta_1x_i)^2})\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\ell(\\beta^*_0, \\beta^*_1) \\geq \\ell(\\beta_0, \\beta_1)\n",
    "$$\n",
    "\n",
    "So $\\beta^*$ is also a maximizer of $\\ell(\\beta_0, \\beta_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Prove the parallelogram identity:\n",
    "\n",
    "$$\n",
    "x^Ty = \\frac{1}{4}\\left(\\Vert x+y\\Vert^2-\\Vert x-y\\Vert^2\\right)\n",
    "$$\n",
    "\n",
    "for all $x,y\\in\\mathbb{R}^d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution to Problem 4\n",
    "\n",
    "$$\n",
    "\\Vert x+y\\Vert^2 - \\Vert x-y\\Vert^2 = (x+y)^T(x+y)-(x-y)^T(x-y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=(x^T+y^T)(x+y)-(x^T-y^T)(x-y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "=x^Tx+y^Tx+x^Ty+y^Ty-x^Tx+x^Ty+y^Tx-y^Ty\n",
    "$$\n",
    "\n",
    "$$\n",
    "=2y^Tx+2x^Ty-4x^Ty+4x^Ty\n",
    "$$\n",
    "\n",
    "$$\n",
    "=2y^Tx-2x^Ty+4x^Ty\n",
    "$$\n",
    "\n",
    "Since $a^Tb=b^Ta$, the original equation becomes $4x^Ty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Another benefit of our reformulation of the $SSE$ function is that it is now dimension independent. That is, if we are given data $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i\\in\\mathbb{R}^d$ and $y_i\\in\\mathbb{R}$ for each $i=1,\\ldots, N$ and we form the higher-order $SSE$ function by defining\n",
    "\n",
    "$$\n",
    "SSE(\\beta_0,\\beta_1,\\ldots, \\beta_d) = \\sum_{i=1}^N(y_i-\\beta_0-\\beta_1x_{i,1}-\\beta_2x_{i,2}-\\cdots-\\beta_d x_{i,d})^2\n",
    "$$\n",
    "\n",
    "with $x_{i,j}$ the $j$th entry of $x_i$, then\n",
    "\n",
    "$$\n",
    "SSE(\\beta) = \\Vert y - X\\beta\\Vert^2\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y=\\begin{pmatrix}y_1\\\\ y_2\\\\\\vdots\\\\ y_N\\end{pmatrix},\\:\\beta=\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_d\n",
    "\\end{pmatrix},\\text{ and } X=\\begin{pmatrix} 1 & x_{1, 1} & x_{1,2} & \\cdots & x_{1,d}\\\\1 & x_{2, 1} & x_{2,2} & \\cdots & x_{2,d}\\\\\\vdots & \\vdots& \\vdots &\\ddots & \\vdots\\\\1 & x_{N, 1} & x_{N,2} & \\cdots & x_{N,d}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Part A\n",
    "\n",
    "Modify the code from class to compute the $\\beta^\\ast\\in\\mathbb{R}^{14}$ (the $0$th entry is the constant $\\beta_0$ and the other entries correpond to the $13$ features of the dataset) solving the least squares problem. Print the $\\beta^\\ast$'s and discuss your findings.\n",
    "\n",
    "### Part B\n",
    "\n",
    "Numerically verify that the $\\beta^\\ast$ satisfies the normal equations. That is, use python code to show that $X^TX\\beta^\\ast - X^T y\\approx 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution to Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n",
      "(506, 13)\n",
      "(506, 14)\n",
      "[ 3.64911033e+01 -1.07170557e-01  4.63952195e-02  2.08602395e-02\n",
      "  2.68856140e+00 -1.77957587e+01  3.80475246e+00  7.51061703e-04\n",
      " -1.47575880e+00  3.05655038e-01 -1.23293463e-02 -9.53463555e-01\n",
      "  9.39251272e-03 -5.25466633e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston # Import the Boston housing dataset loader\n",
    "import matplotlib.pyplot as plt # Import the main plotting library\n",
    "import numpy as np # Import numerical python routines\n",
    "\n",
    "# Load the boston dataset and save as boston\n",
    "boston = load_boston()\n",
    "\n",
    "# Set the responsive variable y as the medium housing price, which is also the target\n",
    "y = boston.target \n",
    "print(y.shape)\n",
    "\n",
    "# Select the predictors as all the other 13 features in the boston dataset\n",
    "x = boston.data[:,0:13] \n",
    "print(x.shape)\n",
    "\n",
    "# Build the design matrix\n",
    "one = np.ones((506,1))\n",
    "X = np.concatenate((one, x), axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "# Use the linear algebra least squares routine to compute the beta's\n",
    "beta = np.linalg.lstsq(X, y)[0] \n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected all 13 features as $x_i$'s and calculated $\\beta^*$ in $\\mathbb{R}^{14}$. Result is shown above. In this $\\beta^*$ vector, the first entry is $\\beta_0$, which represents the intercept on medium housing price, and the entries after that each represent a slope on that particular $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 506)\n",
      "(14,)\n",
      "(14,)\n",
      "[-1.63709046e-11 -9.82254278e-11 -1.28056854e-09 -5.38420863e-10\n",
      " -1.70530257e-12 -1.00044417e-11 -1.60071068e-10 -2.32830644e-09\n",
      " -1.45519152e-10 -2.91038305e-10 -5.58793545e-09 -6.69388101e-10\n",
      " -7.45058060e-09 -3.63797881e-10]\n"
     ]
    }
   ],
   "source": [
    "# Transpose the design matrix\n",
    "X_T = X.transpose()\n",
    "print(X_T.shape)\n",
    "\n",
    "# Calculate X_T*X*beta\n",
    "A = np.matmul(X_T, X)\n",
    "A = np.matmul(A, beta)\n",
    "print(A.shape)\n",
    "\n",
    "# Calculate X_T * y\n",
    "B = np.matmul(X_T, y)\n",
    "print(B.shape)\n",
    "\n",
    "# Show their differences\n",
    "result = A-B\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above we can see that $X^TX\\beta^* - X^Ty$ is *very* small. Therefore, we can say that $\\beta^*$ satisfies the normal equation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
