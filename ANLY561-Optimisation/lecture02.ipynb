{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 02: Linear Geometry and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Euclidean Norm\n",
    "\n",
    "In two dimensions, the length of a vector\n",
    "$$\n",
    "x= \\begin{pmatrix} x_1\\\\ x_2\\end{pmatrix}\\in\\mathbb{R}^2\n",
    "$$\n",
    "is given by\n",
    "$$\n",
    "\\Vert x\\Vert = \\sqrt{x_1^2+x_2^2} = \\sqrt{x^Tx}.\n",
    "$$\n",
    "This follows from the Pythagorean theorem. \n",
    "\n",
    "We can use this to deduce the formula for the length of a vector\n",
    "$$\n",
    "x = \\begin{pmatrix} x_1\\\\ x_2\\\\ x_3\\end{pmatrix}\\in\\mathbb{R}^3.\n",
    "$$\n",
    "The trick is that the triangle with vertices\n",
    "$$\n",
    "\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}, \\begin{pmatrix} x_1 \\\\ x_2\\\\ 0\\end{pmatrix}, \\begin{pmatrix}x_1\\\\ x_2\\\\x_3\\end{pmatrix}\n",
    "$$\n",
    "is a right triangle with side lengths\n",
    "$$\n",
    "\\left\\Vert \\begin{pmatrix} x_1 \\\\ x_2\\\\ x_3\\end{pmatrix} - \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix} \\right\\Vert = \\left\\Vert \\begin{pmatrix} x_1 \\\\ x_2\\\\ x_3\\end{pmatrix}\\right\\Vert,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left\\Vert \\begin{pmatrix} x_1\\\\ x_2\\\\ 0\\end{pmatrix} - \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}\\right\\Vert = \\left\\Vert \\begin{pmatrix} x_1\\\\ x_2\\\\ 0\\end{pmatrix}\\right\\Vert = \\sqrt{x_1^2+x_2^2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left\\Vert \\begin{pmatrix} x_1 \\\\ x_2\\\\ x_3\\end{pmatrix} - \\begin{pmatrix}x_1\\\\x_2\\\\0\\end{pmatrix} \\right\\Vert = \\left\\Vert \\begin{pmatrix} 0 \\\\ 0\\\\ x_3\\end{pmatrix}\\right\\Vert = \\vert x_3\\vert.\n",
    "$$\n",
    "\n",
    "Since $\\Vert x\\Vert$ is the length of the hypotenuse, the Pythagorean theorem gives us\n",
    "\n",
    "$$\n",
    "\\Vert x\\Vert^2 = \\left(\\sqrt{x_1^2+x_2^2}\\right)^2+\\vert x_3\\vert^2 = x_1^2+x_2^2+x_3^2.\n",
    "$$\n",
    "\n",
    "Therefore $\\Vert x\\Vert = \\sqrt{x^Tx}$ in three dimensions.\n",
    "\n",
    "Now, we use the fact that this formula holds in two dimensions to get the formula in three dimensions. This forms the basis for an induction proof that $\\Vert x\\Vert =\\sqrt{x^Tx}$ for all $x\\in\\mathbb{R}^d$.\n",
    "\n",
    "Based on this formula, we note that the general Euclidean norm satisfies the two properties mentioned during the last lecture:\n",
    "\n",
    "1. Positivity: $\\Vert x\\Vert\\geq 0$ for all $x\\in\\mathbb{R}^d$, and $\\Vert x\\Vert=0$ if and only if $x=0$\n",
    "2. Homogeneity: $\\Vert \\alpha x\\Vert=\\vert\\alpha\\vert\\Vert x\\Vert$ for all $\\alpha\\in\\mathbb{R}$ and $x\\in\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Geometric interpretation of Least Squares\n",
    "\n",
    "We note that the general least squares problem\n",
    "\n",
    "$$\n",
    "\\min \\Vert y-X\\beta\\Vert^2\\text{ subject to }\\beta\\in\\mathbb{R}^{d+1}\n",
    "$$\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "$$\n",
    "\\min \\Vert y-X\\beta\\Vert\\text{ subject to }\\beta\\in\\mathbb{R}^{d+1}\n",
    "$$\n",
    "\n",
    "since the square root is an **order preserving transformation**. That is, for any function $f$ which only takes positive values, $f(x)\\leq f(x)$ if and only if $\\sqrt{f(x)}\\leq\\sqrt{f(y)}$. This latter program allows us to interpret least squares geometrically: $\\beta^\\ast$ is a solution if $X\\beta^\\ast$ is as close to $y$ as it possibly can be.\n",
    "\n",
    "We let $col(X)=\\{v\\in\\mathbb{R}^N: v=X\\beta\\:\\text{ for some }\\:\\beta\\in\\mathbb{R}^{d+1}\\}$ denote all the possible values $X\\beta$ could take in $\\mathbb{R}^N$. We note the following important fact about $col(X)$:\n",
    "\n",
    "If $a,b\\in\\mathbb{R}$, and $v, w\\in col(X)$, then $av+bw\\in col(X)$. \n",
    "\n",
    "This means that $col(X)$ is **closed** under scalar multiplication and vector addition. When a subset $\\mathcal{V}\\subset\\mathbb{R}^N$ is closed under these operations, we say that it is a **linear subspace** of $\\mathbb{R}^N$. To summarize, $col(X)$ is a linear subspace of $\\mathbb{R}^N$.\n",
    "\n",
    "Consider the program\n",
    "\n",
    "$$\n",
    "\\min \\Vert y-v\\Vert \\text{ subject to } v\\in col(X).\n",
    "$$\n",
    "\n",
    "This program is generally not equivalent to the original least squares program. This is clear because $\\beta\\in\\mathbb{R}^{d+1}$ and $v\\in \\mathbb{R}^N$, and generally it will not be the case that $d+1=N$. However, if we find a $v^\\ast$ which solves this program, we know that there is a $\\beta^\\ast$ such that $X\\beta^\\ast=v^\\ast$. We will now develop a strategy for producing $v^\\ast$ that also allow us to efficiently produce a $\\beta^\\ast$ that satisfies this equation.\n",
    "\n",
    "### Note: We generally call a solution $v^\\ast$ to a program $\\min \\Vert y-v\\Vert\\text{ subject to } v\\in\\mathcal{X}$ the *projection* of $y$ onto $\\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear combinations\n",
    "\n",
    "A **linear combination** of a collection of vectors $\\{v_i\\}_{i=1}^k\\subset\\mathbb{R}^N$ is any vector of the form\n",
    "\n",
    "$$\n",
    "a_1v_1 + a_2v_2+\\cdots+a_k v_k = \\sum_{i=1}^k a_i v_i\n",
    "$$\n",
    "for some scalars $\\{a_i\\}_{i=1}^k\\subset\\mathbb{R}$. The set of all possible linear combinations of a collection of vectors $\\{v_i\\}_{i=1}^k\\subset\\mathbb{R}^N$ is called the **span** of the collection $\\{v_i\\}_{i=1}^k$, which is denoted\n",
    "\n",
    "$$\n",
    "span(\\{v_i\\}_{i=1}^k) = \\left\\{w\\in\\mathbb{R}^N: w = \\sum_{i=1}^k a_i v_i\\text{ for some } \\{a_i\\}_{i=1}^k\\subset\\mathbb{R}\\right\\}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Recalling $X$ from our least squares section, note that $col(X) = span(\\{X_{\\cdot,j}\\}_{j=1}^{d+1})$ where $X_{\\cdot,j}$ is the $j$th column of $X$:\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix} X_{\\cdot, 1} & X_{\\cdot, 2} &\\cdots & X_{\\cdot, d+1}\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "That is, $col(X)$ is the span of the columns of $X$, which we call the **column space** of $X$.\n",
    "\n",
    "### Linear subspace property of the span\n",
    "\n",
    "Let $p, q\\in span(\\{v_i\\}_{i=1}^k)$ for some collection $\\{v_i\\}_{i=1}^k\\subset\\mathbb{R}^N$. Then there are scalar collections $\\{a_i\\}_{i=1}^k, \\{b_i\\}_{i=1}^k\\subset\\mathbb{R}$ such that\n",
    "$$\n",
    "p = \\sum_{i=1}^k a_i v_i\\text{ and } q = \\sum_{i=1}^k b_i v_i.\n",
    "$$\n",
    "For any $\\alpha,\\beta\\in\\mathbb{R}$, we have\n",
    "\\begin{align}\n",
    "\\alpha p + \\beta q &= \\alpha\\left(\\sum_{i=1}^k a_i v_i\\right)+\\beta\\left(\\sum_{i=1}^k b_iv_i\\right)\\\\\n",
    "&=\\sum_{i=1}^k \\alpha a_i v_i+\\sum_{i=1}^k \\beta b_iv_i\\\\\n",
    "&=\\sum_{i=1}^k (\\alpha a_i v_i+\\beta b_iv_i)\\\\\n",
    "&=\\sum_{i=1}^k (\\alpha a_i+\\beta b_i)v_i\\\\\n",
    "&=\\sum_{i=1}^k c_iv_i\\\\\n",
    "\\end{align}\n",
    "where we have set $c_i = \\alpha a_i + \\beta b_i$ for $i=1,\\ldots, k$. Since $\\{c_i\\}_{i=1}^k$ is a collection of scalars, we conclude that $\\alpha p + \\beta q\\in span(\\{v_i\\}_{i=1}^k)$. Therefore, $span(\\{v_i\\}_{i=1}^k)$ is a linear subspace of $\\mathbb{R}^N$. \n",
    "\n",
    "#### Note: The set containing just the zero vector $\\{0\\}$ is called the *trivial* subspace of $\\mathbb{R}^N$, and any subspace $\\mathcal{V}\\not=\\{0\\}$ of $\\mathbb{R}^N$ is said to be *non-trivial*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary and Sufficient Conditions for Projection onto Subspaces\n",
    "\n",
    "Let's revist the program\n",
    "\n",
    "$$\n",
    "\\min \\Vert y - v\\Vert^2 \\text{ subject to } v\\in\\mathcal{V}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{V}$ is a subspace of $\\mathbb{R}^N$. We claim that the following condition is necessary and sufficient for a solution $v^\\ast$:\n",
    "\n",
    "$$\n",
    "(y-v)^Tw=0\\text{ for all }w\\in\\mathcal{V}.\n",
    "$$\n",
    "\n",
    "### Sufficiency\n",
    "\n",
    "Suppose $(y-v)^Tw=0$ for all $w\\in\\mathcal{V}$, and let $v\\in\\mathcal{V}$. Then\n",
    "\n",
    "\\begin{align}\n",
    "\\Vert y-v\\Vert^2 &=  \\Vert y-v^\\ast-(v-v^\\ast)\\Vert^2\\\\\n",
    "&= \\Vert y-v^\\ast\\Vert^2-2(y-v^\\ast)^T(v-v^\\ast)+\\Vert v-v^\\ast\\Vert^2\\\\\n",
    "&= \\Vert y-v^\\ast\\Vert^2+\\Vert v-v^\\ast\\Vert^2\\\\\n",
    "&\\geq \\Vert y-v^\\ast\\Vert^2.\n",
    "\\end{align}\n",
    "\n",
    "Thus, $v^\\ast$ is a minimizer of this program over $\\mathcal{V}$.\n",
    "\n",
    "### Necessity\n",
    "\n",
    "To establish that, if $v^\\ast$ is a minimizer, then $(y-v^\\ast)^Tw=0$ for all $w\\in\\mathcal{V}$, we will instead prove the contrapositive. That is, we will show that, if there is *some* $w\\in\\mathcal{V}$ such that $(y-v^\\ast)^Tw\\not=0$, then $v^\\ast$ is not a minimizer. \n",
    "\n",
    "Let $w\\in\\mathcal{V}$ be such that $(y-v^\\ast)^Tw\\not=0$, and note that this implies $w\\not=0$ and $\\Vert w\\Vert^2\\not=0$. If we let $\\delta=(y-v^\\ast)^Tw/\\Vert w\\Vert^2$, then \n",
    "\n",
    "\\begin{align}\n",
    "\\Vert y-(v^\\ast+\\delta w)\\Vert^2 &=  \\Vert (y-v^\\ast) - \\delta w\\Vert^2\\\\\n",
    "&= \\Vert y-v^\\ast\\Vert^2-2\\delta(y-v^\\ast)^Tw+\\delta^2\\Vert w\\Vert^2\\\\\n",
    "&= \\Vert y-v^\\ast\\Vert^2-\\frac{\\vert (y-v^\\ast)^Tw\\vert^2}{\\Vert w\\Vert^2}\\\\\n",
    "&< \\Vert y-v^\\ast\\Vert^2.\n",
    "\\end{align}\n",
    "\n",
    "Now, $v^\\ast+\\delta w\\in\\mathcal{V}$ since $\\mathcal{V}$ is a linear subspace of $\\mathbb{R}^N$, so we conclude that $v^\\ast$ is not a minimizer of $\\Vert y-v\\Vert^2$ over $\\mathcal{V}$.\n",
    "\n",
    "### Orthogonality\n",
    "\n",
    "Whenever $q^Tp=0$ we say that $q$ and $p$ are **orthogonal**. Whenever $q^Tp=0$ for all $p\\in\\mathcal{V}$, we say that $q$ is **orthogonal** to $\\mathcal{V}$. So now we can say that necessary and sufficient conditions for solving this program require $y-v^\\ast$ to be orthogonal to $\\mathcal{V}$. Because of this necessary and sufficient condition, we say that a solution $v^\\ast$ is the **orthogonal projection** of $y$ onto $\\mathcal{V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthonormal Bases\n",
    "\n",
    "We now define orthonormal bases and show how they allow us to efficiently compute projections onto subspaces. \n",
    "\n",
    "### Linear span of a vector collection\n",
    "\n",
    "A collection $\\{v_i\\}_{i=1}^k\\subset\\mathbb{R}^N$ is said to **span** a linear subspace $\\mathcal{V}\\subset\\mathbb{R}^N$ if\n",
    "\n",
    "$$\n",
    "span(\\{v_i\\}_{i=1}^k) = \\mathcal{V}.\n",
    "$$\n",
    "\n",
    "In particular, this means that each member of $\\mathcal{V}$ is a linear combination of the $v_i$'s, and any linear combination of the $v_i$'s is a member of $\\mathcal{V}$. \n",
    "\n",
    "### Orthogonal collections\n",
    "\n",
    "A collection $\\{v_i\\}_{i=1}^k$ is said to be **orthogonal** if $v_i^Tv_j=0$ for any $i\\not=j$. A vector $u$ with $\\Vert u\\Vert=1$ is said to be a **unit vector** or **normalized**. A collection $\\{u_i\\}_{i=1}^k$ is said to be **normalized** if $u_i^Tu_i=1$ for all $i=1,\\ldots, k$. We say that a collection $\\{u_i\\}_{i=1}^k$ is **orthonormal** if it is an orthogonal and normalized collection. If $\\{u_i\\}_{i=1}^k$ is an orthonormal collection and $span(\\{u_i\\}_{i=1}^k)=\\mathcal{V}$, then we say that $\\{u_i\\}_{i=1}^k$ is an **orthonormal basis** or **ONB** of $\\mathcal{V}$\n",
    "\n",
    "\n",
    "### The canonical orthonormal basis of $\\mathbb{R}^n$\n",
    "\n",
    "We define the collection $\\{e_i\\}_{i=1}^n$ by\n",
    "\n",
    "$$\n",
    "e_1 = \\begin{pmatrix}\n",
    "1\\\\ 0\\\\0\\\\\\vdots\\\\ 0\\\\0\\\\0\n",
    "\\end{pmatrix},\\:e_2 = \\begin{pmatrix}\n",
    "0\\\\ 1\\\\0\\\\\\vdots\\\\ 0\\\\0\\\\0\n",
    "\\end{pmatrix},\\ldots,\\: e_{n-1} = \\begin{pmatrix}\n",
    "0\\\\ 0\\\\0\\\\\\vdots\\\\ 0\\\\1\\\\0\n",
    "\\end{pmatrix},\\: e_{n} = \\begin{pmatrix}\n",
    "0\\\\ 0\\\\0\\\\\\vdots\\\\ 0\\\\0\\\\1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This is the most self-evident orthonormal basis of $\\mathbb{R}^n$, and we call it the **canonical ONB** of $\\mathbb{R}^n$. Whenever the symbol $e_i$ appears in future course content, it is the $i$th member of the canonical ONB.\n",
    "\n",
    "In this basis, $x\\in\\mathbb{R}^n$ may be written $x=\\sum_{i=1}^n x_i e_i$, where $x_i$ is the $i$th entry of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy computation of unique representations for orthonormal bases\n",
    "\n",
    "The nice property about an orthonormal basis $\\{u_i\\}_{i=1}^k$ for a subspace $\\mathcal{V}$ is that there are unique coefficients $c_i$ which satisfy\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^k c_i u_i\n",
    "$$\n",
    "\n",
    "for any $w\\in\\mathcal{V}$, and $c_i=u_i^T w$:\n",
    "\n",
    "Let $w\\in\\mathcal{V}$. Since $\\{u_i\\}_{i=1}^k$ is an orthonormal basis of $\\mathcal{V}$, it spans $\\mathcal{V}$, so there are coefficients $c_i$ such that $w = \\sum_{i=1}^k c_i u_i$. Then\n",
    "\n",
    "$$\n",
    "u_i^T w = u_i^T\\left(\\sum_{j=1}^k c_j u_j\\right)=\\sum_{j=1}^k c_j u_i^Tu_j=c_i u_i^Tu_i +\\sum_{j\\not=i} c_j u_i^Tu_j=c_i (1) +\\sum_{j\\not=i} c_j (0)=c_i.\n",
    "$$\n",
    "\n",
    "Thus, it is standard to write\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^k (u_i^T w) u_i\n",
    "$$\n",
    "\n",
    "if $w\\in\\mathcal{V}$ and $\\{u_i\\}_{i=1}^k$ is an orthonormal basis for $\\mathcal{V}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy computation of orthogonal projections using orthonormal bases\n",
    "\n",
    "If $\\{u_i\\}_{i=1}^k$ is an orthonormal basis for the subspace $\\mathcal{V}\\subset\\mathbb{R}^N$ and $y\\in\\mathbb{R}^N$, we claim that\n",
    "\n",
    "$$\n",
    "v^\\ast = \\sum_{i=1}^k (u_i^T y) u_i\n",
    "$$\n",
    "\n",
    "is the orthonormal projection of $y$ onto $\\mathcal{V}$. Suppose $w\\in \\mathcal{V}$. Then $w = \\sum_{i=1}^k(u_i^Tw)u_i$ and \n",
    "\n",
    "\\begin{align}\n",
    "(y-v^\\ast)^T w &= \\left(y-\\sum_{i=1}^k (u_i^T y) u_i\\right)^T\\left(\\sum_{j=1}^k(u_j^Tw)u_j\\right)\\\\\n",
    "&=\\left(y^T-\\sum_{i=1}^k (u_i^T y) u_i^T\\right)\\left(\\sum_{j=1}^k(u_j^Tw)u_j\\right)\\\\\n",
    "&=y^T\\left(\\sum_{j=1}^k(u_j^Tw)u_j\\right)-\\sum_{i=1}^k (u_i^T y) u_i^T\\left(\\sum_{j=1}^k(u_j^Tw)u_j\\right)\\\\\n",
    "&=\\left(\\sum_{j=1}^k(u_j^Tw)y^Tu_j\\right)-\\sum_{i=1}^k\\sum_{j=1}^k (u_i^T y)(u_j^Tw) u_i^Tu_j\\\\\n",
    "&=\\left(\\sum_{j=1}^k(u_j^Tw)y^Tu_j\\right)-\\sum_{i=1}^k(u_i^T y)(u_i^Tw) u_i^Tu_i - \\sum_{i\\not=j}(u_i^T y)(u_j^Tw) u_i^Tu_j\\\\\n",
    "&=\\left(\\sum_{j=1}^k(u_j^Tw)y^Tu_j\\right)-\\sum_{i=1}^k(u_i^T y)(u_i^Tw)(1) - \\sum_{i\\not=j}(u_i^T y)(u_j^Tw) (0)\\\\\n",
    "&=\\left(\\sum_{j=1}^k(u_j^Tw)y^Tu_j\\right)-\\sum_{i=1}^k(u_i^T y)(u_i^Tw)\\\\\n",
    "&=0.\n",
    "\\end{align}\n",
    "\n",
    "Therefore, $v^\\ast$ satisfies the necessary and sufficient conditions for being the orthogonal projection of $y$ onto $\\mathcal{V}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing an Orthonormal Basis\n",
    "\n",
    "One way to construct an orthonormal basis of $\\mathcal{V}$ from a collection $\\{v_i\\}_{i=1}^k$ with $span(\\{v_i\\}_{i=1}^k)=\\mathcal{V}$ is via the **Gramm-Schmidt procedure**. First, we set\n",
    "\n",
    "$$\n",
    "u_1 = \\frac{1}{\\Vert v_1\\Vert}v_1 = r_{1,\\:1} v_1,\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_2 = \\frac{1}{\\Vert v_2 - (v_2^T u_1)u_1\\Vert} \\left(v_2 - (v_2^T u_1)u_1\\right) = r_{1,\\:2} v_1 + r_{2,\\:2} v_2,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_3 = \\frac{1}{\\Vert v_3 - (v_3^T u_1)u_1-(v_3^Tu_2)u_2\\Vert} \\left(v_3 - (v_3^T u_1)u_1-(v_3^Tu_2)u_2\\right)=r_{1,\\: 3}v_1 + r_{2,\\:3} v_2 + r_{3,\\:3} v_3.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In general,\n",
    "\n",
    "\\begin{align}\n",
    "u_i = \\frac{1}{\\Vert v_i - \\sum_{j< i} (v_i^T u_j) u_j\\Vert}\\left(v_i - \\sum_{j< i} (v_i^T u_j) u_j \\right) = \\sum_{j\\leq i} r_{j,\\: i} v_j\n",
    "\\end{align}\n",
    "\n",
    "The way to understand this is that we iteratively compute $v_i - p_i$ where $p_i$ is the orthogonal projection of $v_i$ onto $span(\\{u_j\\}_{j<i})$, and then **normalize** by dividing by the norm of $v_i-p_i$. Note that $v_i-p_i$ is orthogonal to $span(\\{u_j\\}_{j<i})$ since $p_i$ is the orthogonal projection. This means that\n",
    "\n",
    "$$\n",
    "u_i^Tu_j=0\\text{ for all } i<j.\n",
    "$$\n",
    "\n",
    "and the orthogonality conditions are ensured. \n",
    "\n",
    "By construction, $\\{u_i\\}_{i=1}^k$ is an orthonormal collection. Also note that\n",
    "$$\n",
    "v_i = \\Vert v_i - \\sum_{j< i} (v_i^T u_j) u_j\\Vert u_i + \\sum_{j< i} (v_i^T u_j) u_j,\n",
    "$$\n",
    "\n",
    "so $v_i\\in span(\\{u_j\\}_{j\\leq i})\\subset span(\\{u_j\\}_{j=1}^k$, and we conclude that $span(\\{u_j\\}_{j=1}^k)=span(\\{v_i\\}_{i=1}^k)$. Thus, $\\{u_i\\}_{i=1}^k$ is an orthonormal basis for the span of $\\{v_i\\}_{i=1}^k$.\n",
    "\n",
    "#### Note: if $v_i-p_i=0$, then normalization is impossible. In this case, we set $u_i=0$ and $r_{i,\\:j}=0$ for $j\\leq i$. When we do this, we will have that $\\{u_i\\}_{i=1}^k$ consists of an orthonormal basis and zero vectors. The important thing to note is that $v=\\sum_{i=1}^k (u_i^T v)u_i$ will still hold for $v\\in\\mathcal{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Least Squares Problems with an Orthonormal Basis\n",
    "\n",
    "For a response vector $y\\in\\mathbb{R}^N$ and a design matrix $X\\in\\mathbb{R}^{N\\times(d+1)}$, we perform Gramm-Schmidt on the columns of $X$ to obtain an orthonormal basis $\\{u_i\\}_{i=1}^{d+1}$ for $col(X)$. The orthogonal projection of $y$ onto $col(X)$ is given by\n",
    "\n",
    "\\begin{align}\n",
    "v^\\ast &= \\sum_{i=1}^k (u_i^Ty)u_i\\\\\n",
    "&=\\sum_{i=1}^{k}(u_i^Ty)\\left(\\sum_{j\\leq i} r_{i, \\:j} X_{\\cdot,\\:j}\\right)\\\\\n",
    "&= \\sum_{j=1}^{d+1}\\left(\\sum_{i\\geq j} (u_i^T y) r_{i,\\: j}\\right) X_{\\cdot,\\:j}\\\\\n",
    "&= XRU^Ty\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "U=\\begin{pmatrix}u_1 & u_2 &\\cdots & u_k\\end{pmatrix}\\text{ and }R=\\begin{pmatrix} r_{1,\\: 1} & r_{1,\\:2} & \\cdots & r_{1,\\:d} & r_{1,\\:d+1}\\\\ 0 & r_{2,\\:2} & \\cdots & r_{2,\\:d} & r_{2,\\:d+1}\\\\ \\vdots & \\vdots & \\ddots &\\vdots & \\vdots\\\\\n",
    "0 & 0 & \\cdots & r_{d,\\: d} & r_{d,\\:d+1}\\\\ 0 & 0 & \\cdots & 0 & r_{d+1,\\:d+1}\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Then $\\beta^\\ast=RU^Ty$ satisfies $X\\beta^\\ast=v^\\ast$, so $\\beta^\\ast$ is a solution to the least squares problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Suppose we have the design matrix and response vector\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "1 & -1 & 0 & 1\\\\\n",
    "1 & 1 & 1 & 1\\\\\n",
    "1 & -1 & 0 &-1\\\\\n",
    "1 & 1 & 1 & -1\\\\\n",
    "1 & 1 & 1 & -1\n",
    "\\end{pmatrix}\\text{ and }y=\\begin{pmatrix}\n",
    "1\\\\ 1\\\\ 0\\\\ 2 \\\\ 3 \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We apply Gramm-Schmidt to the columns of $X$:\n",
    "\n",
    "$$\n",
    "u_1 = \\frac{1}{\\Vert X_{\\cdot,\\:1}\\Vert}X_{\\cdot,\\: 1}=\\frac{1}{\\sqrt{1^2+1^2+1^2+1^2+1^2}}\\begin{pmatrix}1\\\\1\\\\1\\\\1\\\\1\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We then compute\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_{\\cdot,\\:2}-(u_1^TX_{\\cdot,\\:2})u_1&=\\begin{pmatrix}-1\\\\1\\\\-1\\\\1\\\\1\\end{pmatrix}-\\left(\\frac{1}{\\sqrt{5}}(-1)+\\frac{1}{\\sqrt{5}}(1)+\\frac{1}{\\sqrt{5}}(-1)+\\frac{1}{\\sqrt{5}}(1)+\\frac{1}{\\sqrt{5}}(1)\\right)\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}-1\\\\1\\\\-1\\\\1\\\\1\\end{pmatrix}-\\frac{1}{\\sqrt{5}}\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}-1\\\\1\\\\-1\\\\1\\\\1\\end{pmatrix} - \\begin{pmatrix}\\frac{1}{5}\\\\\\frac{1}{5}\\\\\\frac{1}{5}\\\\\\frac{1}{5}\\\\\\frac{1}{5}\\end{pmatrix} = \\begin{pmatrix}-\\frac{6}{5}\\\\\\frac{4}{5}\\\\-\\frac{6}{5}\\\\\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\Vert X_{\\cdot,\\:2}-(u_1^TX_{\\cdot,\\:2})u_1\\Vert = \\sqrt{\\left(-\\frac{6}{5}\\right)^2+\\left(\\frac{4}{5}\\right)^2+\\left(-\\frac{6}{5}\\right)^2+\\left(\\frac{4}{5}\\right)^2+\\left(\\frac{4}{5}\\right)^2}=\\frac{2\\sqrt{30}}{5}.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_2 &= \\frac{1}{\\Vert X_{\\cdot,\\:2}-(u_1^TX_{\\cdot,\\:2})u_1\\Vert}\\left(X_{\\cdot,\\:2}-(u_1^TX_{\\cdot,\\:2})u_1\\right)\\\\\n",
    "&= \\frac{5}{2\\sqrt{30}}\\begin{pmatrix}-\\frac{6}{5}\\\\\\frac{4}{5}\\\\-\\frac{6}{5}\\\\\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}\\\\\n",
    "&= \\begin{pmatrix}-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we also note that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_2 &= \\frac{5}{2\\sqrt{30}}X_{\\cdot,\\:2}-\\frac{5}{2\\sqrt{30}}\\left(\\frac{1}{\\sqrt{5}}\\right)\\left(\\frac{1}{\\sqrt{5}}\\right)X_{\\cdot,\\:1}\\\\\n",
    "&= - \\frac{1}{2\\sqrt{30}}X_{\\cdot,\\:1}+\\frac{5}{2\\sqrt{30}}X_{\\cdot,\\:2} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Next, we observe that\n",
    "\n",
    "$$\n",
    "(u_1^T X_{\\cdot,\\: 3})u_1= \\frac{3}{\\sqrt{5}}\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}=\\begin{pmatrix}\\frac{3}{5}\\\\\\frac{3}{5}\\\\\\frac{3}{5}\\\\\\frac{3}{5}\\\\\\frac{3}{5}\\end{pmatrix}\\text{ and }(u_2^T X_{\\cdot,\\: 3})u_2=\\frac{6}{\\sqrt{30}}\\begin{pmatrix}-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\end{pmatrix}=\\begin{pmatrix}-\\frac{3}{5}\\\\\\frac{2}{5}\\\\-\\frac{3}{5}\\\\\\frac{2}{5}\\\\\\frac{2}{5}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "so $X_{\\cdot,3}-(u_1^T X_{\\cdot,\\: 3})u_1-(u_2^T X_{\\cdot,\\: 3})u_2=0$, so we skip this column. For the final column, we have\n",
    "\n",
    "$$\n",
    "(u_1^T X_{\\cdot,\\: 4})u_1=-\\frac{1}{\\sqrt{5}}\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}=\\begin{pmatrix}-\\frac{1}{5}\\\\-\\frac{1}{5}\\\\-\\frac{1}{5}\\\\-\\frac{1}{5}\\\\-\\frac{1}{5}\\end{pmatrix}=-\\frac{1}{5} X_{\\cdot,\\:1}\\text{ and }(u_2^T X_{\\cdot,\\: 4})u_2=-\\frac{2}{\\sqrt{30}}\\begin{pmatrix}-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\end{pmatrix}=\\begin{pmatrix}\\frac{6}{30}\\\\-\\frac{4}{30}\\\\\\frac{6}{30}\\\\-\\frac{4}{30}\\\\-\\frac{4}{30}\\end{pmatrix}=\\frac{1}{30}X_{\\cdot,\\:1}-\\frac{5}{30}X_{\\cdot,\\:2} \n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "X_{\\cdot,\\:4}-(u_1^T X_{\\cdot,\\: 4})u_1-(u_2^T X_{\\cdot,\\: 4})u_2=\\begin{pmatrix}1\\\\1\\\\-1\\\\-1\\\\-1\\end{pmatrix}-\\begin{pmatrix}-\\frac{1}{5}\\\\-\\frac{1}{5}\\\\-\\frac{1}{5}\\\\-\\frac{1}{5}\\\\-\\frac{1}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{6}{30}\\\\-\\frac{4}{30}\\\\\\frac{6}{30}\\\\-\\frac{4}{30}\\\\-\\frac{4}{30}\\end{pmatrix}=\\begin{pmatrix}1\\\\\\frac{4}{3}\\\\-1\\\\-\\frac{2}{3}\\\\-\\frac{2}{3}\\end{pmatrix}=\\frac{1}{6}X_{\\cdot,\\:1}+\\frac{1}{6}X_{\\cdot,\\:2}+X_{\\cdot,\\:4}.\n",
    "$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$\n",
    "\\Vert X_{\\cdot,\\:4}-(u_1^T X_{\\cdot,\\: 4})u_1-(u_2^T X_{\\cdot,\\: 4})u_2\\Vert = \\sqrt{(1)^2 + \\left(\\frac{4}{3}\\right)^2+(-1)^2+\\left(-\\frac{2}{3}\\right)^2+\\left(-\\frac{2}{3}\\right)^2}=\\sqrt{\\frac{42}{9}}=\\frac{\\sqrt{42}}{3}\n",
    "$$\n",
    "\n",
    "to conclude that\n",
    "\n",
    "$$\n",
    "u_3 = \\frac{1}{\\Vert X_{\\cdot,\\:4}-(u_1^T X_{\\cdot,\\: 4})u_1-(u_2^T X_{\\cdot,\\: 4})u_2\\Vert}\\left( X_{\\cdot,\\:4}-(u_1^T X_{\\cdot,\\: 4})u_1-(u_2^T X_{\\cdot,\\: 4})u_2\\right)=\\begin{pmatrix}\\frac{3}{\\sqrt{42}}\\\\\\frac{4}{\\sqrt{42}}\\\\-\\frac{3}{\\sqrt{42}}\\\\-\\frac{2}{\\sqrt{42}}\\\\-\\frac{2}{\\sqrt{42}}\\end{pmatrix}=\\frac{1}{2\\sqrt{42}}X_{\\cdot,\\:1}+\\frac{1}{2\\sqrt{42}}X_{\\cdot,\\:2}+\\frac{3}{\\sqrt{42}}X_{\\cdot,\\:4}.\n",
    "$$\n",
    "\n",
    "We have the following orthonormal basis for the column space of $X$:\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}, \\begin{pmatrix}-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\end{pmatrix},\\:\\begin{pmatrix}\\frac{3}{\\sqrt{42}}\\\\\\frac{4}{\\sqrt{42}}\\\\-\\frac{3}{\\sqrt{42}}\\\\-\\frac{2}{\\sqrt{42}}\\\\-\\frac{2}{\\sqrt{42}}\\end{pmatrix}\\right\\}\n",
    "$$\n",
    "\n",
    "We now compute the projection of $y$ onto the column space:\n",
    "\n",
    "$$\n",
    "(u_1^Ty)u_1 = \\frac{7}{\\sqrt{5}}\\begin{pmatrix}\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\\\\\frac{1}{\\sqrt{5}}\\end{pmatrix}=\\begin{pmatrix}\\frac{7}{5}\\\\\\frac{7}{5}\\\\\\frac{7}{5}\\\\\\frac{7}{5}\\\\\\frac{7}{5}\\end{pmatrix}=\\frac{7}{5}X_{\\cdot,\\:1},\n",
    "$$\n",
    "$$\n",
    "(u_2^Ty)u_2 = \\frac{9}{\\sqrt{30}}\\begin{pmatrix}-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\-\\frac{3}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\\\\\frac{2}{\\sqrt{30}}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{10}\\\\\\frac{6}{10}\\\\-\\frac{9}{10}\\\\\\frac{6}{10}\\\\\\frac{6}{10}\\end{pmatrix}=- \\frac{3}{20}X_{\\cdot,\\:1}+\\frac{15}{20}X_{\\cdot,\\:2} ,\n",
    "$$\n",
    "$$\n",
    "(u_3^Ty)u_3 = -\\frac{3}{\\sqrt{42}}\\begin{pmatrix}\\frac{3}{\\sqrt{42}}\\\\\\frac{4}{\\sqrt{42}}\\\\-\\frac{3}{\\sqrt{42}}\\\\-\\frac{2}{\\sqrt{42}}\\\\-\\frac{2}{\\sqrt{42}}\\end{pmatrix}=\\begin{pmatrix}-\\frac{3}{14}\\\\-\\frac{2}{7}\\\\\\frac{3}{14}\\\\\\frac{1}{7}\\\\\\frac{1}{7}\\end{pmatrix}=-\\frac{1}{28}X_{\\cdot,\\:1}-\\frac{1}{28}X_{\\cdot,\\:2}-\\frac{3}{14}X_{\\cdot,\\:4}\n",
    "$$\n",
    "\n",
    "We conclude that\n",
    "\n",
    "$$\n",
    "v^\\ast = (u_1^Ty)u_1+(u_2^Ty)u_2+(u_3^Ty)u_3=\\begin{pmatrix}\\frac{2}{7}\\\\\\frac{12}{7}\\\\\\frac{5}{7}\\\\\\frac{15}{7}\\\\\\frac{15}{7}\\end{pmatrix}=\\frac{17}{14}X_{\\cdot,\\:1}+\\frac{10}{14}X_{\\cdot,\\:2}-\\frac{3}{14}X_{\\cdot,\\:4},\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\beta^\\ast = \\begin{pmatrix}\\frac{17}{14}\\\\\\frac{10}{14}\\\\0\\\\-\\frac{3}{14}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is a solution to the least squares problem with this design matrix and response vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "1. Least squares approximates responses by projecting the measured responses onto the column space of the design matrix.\n",
    "2. Projections onto subspaces are easily computed using orthonormal bases\n",
    "3. Orthonormal bases for a column space can be constructed using the Gramm-Schmidt process.\n",
    "4. We can recover the coordinates of a vector in the column space of a matrix $X$ by retaining the coefficients of an orthonormal basis in terms of the columns of $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
